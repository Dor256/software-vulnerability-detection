# Models
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline, make_pipeline
# Sampling
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
# Plotting
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, plot_precision_recall_curve, plot_roc_curve, plot_confusion_matrix
# IO
import pandas as pd
import pickle

# Read dataset into memory
data = pd.read_csv("./JSVulnerabilityDataSet-1.0.csv")
parametersIndex = data.columns.get_loc("CC")
resultIndex = data.columns.get_loc("Vuln")
X = data.iloc[:, parametersIndex:resultIndex]
Y = data.iloc[:, resultIndex]

# Prepare plots
fig_confusion, subs_confusion = plt.subplots(3,3)
fig_metrics, subs_metrics = plt.subplots(3,3)
subs_metrics[0][0].set_title('precision vs recall')
subs_metrics[0][1].set_title('ROC')
subs_metrics[0][2].set_title('Accuracy')
for idx, row in enumerate(subs_metrics):
    row[0].set_yticklabels([])
    row[0].set_xticklabels([])
    row[0].get_xaxis().set_visible(False)
    row[1].set_yticklabels([])
    row[1].set_xticklabels([])
    row[1].get_yaxis().set_visible(False)
    row[1].get_xaxis().set_visible(False)
    row[2].set_yticklabels([])
    row[2].set_xticklabels([])
    row[2].get_yaxis().set_visible(False)
    row[2].get_xaxis().set_visible(False)

row = 0

def train_model(model_type, X_train, Y_train, sampling):
    if sampling != "no":
        sampler = RandomOverSampler() if sampling == "over" else RandomUnderSampler()
        X_train, Y_train = sampler.fit_resample(X_train, Y_train)
    trained_model = make_pipeline(StandardScaler(), model_type)
    trained_model.fit(X_train, Y_train)

    return trained_model

def plot_metric(title, metric_plotter, trained_model, X_test, Y_test, row, col):
    plot_configuration = metric_plotter(trained_model, X_test, Y_test)
    plot_configuration.plot(ax=subs_metrics[row][col], name=title)

models = {
    "Logistic Regression": LogisticRegression(solver='newton-cg'),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier()
}

for model_name, model_type in models.items():
    # Split the data for training and validation
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y)
    
    # Train the model with different sampling strategies
    model_over_sampling = train_model(model_type, X_train, Y_train, "over")
    model_under_sampling = train_model(model_type, X_train, Y_train, "under")
    model_no_sampling = train_model(model_type, X_train, Y_train, "no")

    # Report confusion matrices
    subs_confusion[row][0].set_title('no sampling')
    confusion_plot_no = plot_confusion_matrix(model_no_sampling, X_test, Y_test, normalize='true')
    confusion_plot_no.plot(ax=subs_confusion[row][0])

    subs_confusion[row][1].set_title('over sampling')
    confusion_plot_over = plot_confusion_matrix(model_over_sampling, X_test, Y_test, normalize='true')
    confusion_plot_over.plot(ax=subs_confusion[row][1])

    subs_confusion[row][2].set_title('under sampling')
    confusion_plot_under = plot_confusion_matrix(model_under_sampling, X_test, Y_test, normalize='true')
    confusion_plot_under.plot(ax=subs_confusion[row][2])
    
    # Report precision recall curve
    plot_metric('no sampling', plot_precision_recall_curve, model_no_sampling, X_test, Y_test, row, 0)
    plot_metric('over sampling', plot_precision_recall_curve, model_over_sampling, X_test, Y_test, row, 0)
    plot_metric('under sampling', plot_precision_recall_curve, model_under_sampling, X_test, Y_test, row, 0)

    # Report ROC curve
    plot_metric('no sampling', plot_roc_curve, model_no_sampling, X_test, Y_test, row, 1)
    plot_metric('over sampling', plot_roc_curve, model_over_sampling, X_test, Y_test, row, 1)
    plot_metric('under sampling', plot_roc_curve, model_under_sampling, X_test, Y_test, row, 1)

    # Report accuracy
    prediction_no_sampling = model_no_sampling.predict(X_test)
    prediction_over_sampling = model_over_sampling.predict(X_test)
    prediction_under_sampling = model_under_sampling.predict(X_test)

    accuracy_no_sampling = accuracy_score(Y_test, prediction_no_sampling)
    accuracy_over_sample = accuracy_score(Y_test, prediction_over_sampling)
    accuracy_under_sample = accuracy_score(Y_test, prediction_under_sampling)
    subs_metrics[row][2].bar(['no sampling'], [accuracy_no_sampling], color='b')
    subs_metrics[row][2].bar(['over sampling'], [accuracy_over_sample], color='orange')
    subs_metrics[row][2].bar(['under sampling'], [accuracy_under_sample], color='g')
    subs_metrics[row][2].legend([
        "no sampling - {:.0%}".format(accuracy_no_sampling),
        "over sampling - {:.0%}".format(accuracy_over_sample),
        "under sampling - {:.0%}".format(accuracy_under_sample)
    ])

    # Add model name to plots row
    subs_metrics[row][0].set_ylabel(model_name)


    # Clear un-needed figures to coserve memory
    for fig_num in plt.get_fignums():
        if fig_confusion.number != fig_num and fig_metrics.number != fig_num:
            plt.close(fig_num)

    # Save model result to file
    pickle.dump(model_no_sampling, open('./models/' + model_name.lower().replace(' ', '_') + '_no_sampling', 'wb'))
    pickle.dump(model_over_sampling, open('./models/' + model_name.lower().replace(' ', '_') + '_over_sampling', 'wb'));
    pickle.dump(model_under_sampling, open('./models/' + model_name.lower().replace(' ', '_') + '_under_sampling', 'wb'));
    row += 1

plt.show()

