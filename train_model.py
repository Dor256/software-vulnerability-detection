# Models
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline, make_pipeline
# Sampling
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
# Plotting
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score, plot_precision_recall_curve, plot_roc_curve, plot_confusion_matrix
# IO
import numpy as np
import pandas as pd
import pickle

pd.options.display.float_format = "{:,.2f}".format

# Read dataset into memory
data = pd.read_csv("./JSVulnerabilityDataSet-1.0.csv")
parametersIndex = data.columns.get_loc("CC")
resultIndex = data.columns.get_loc("Vuln")
X = data.iloc[:, parametersIndex:resultIndex]
Y = data.iloc[:, resultIndex]

# X = SelectKBest(chi2, k=20).fit_transform(X, Y)
# print(X_new)

# Prepare plots
fig_confusion, subs_confusion = plt.subplots(3,3)
fig_metrics, subs_metrics = plt.subplots(3,3)
subs_metrics[0][0].set_title('precision vs recall')
subs_metrics[0][1].set_title('ROC')
subs_metrics[0][2].set_title('Accuracy')
subs_confusion[0][0].set_title('no sampling')
subs_confusion[0][1].set_title('over sampling')
subs_confusion[0][2].set_title('under sampling')
for idx, row in enumerate(subs_metrics):
    row[0].set_yticklabels([])
    row[0].set_xticklabels([])
    row[0].get_xaxis().set_visible(False)
    row[1].set_yticklabels([])
    row[1].set_xticklabels([])
    row[1].get_yaxis().set_visible(False)
    row[1].get_xaxis().set_visible(False)
    row[2].set_yticklabels([])
    row[2].set_xticklabels([])
    row[2].get_yaxis().set_visible(False)
    row[2].get_xaxis().set_visible(False)

for idx, row in enumerate(subs_confusion):
    row[0].set_yticklabels([])
    row[0].set_xticklabels([])
    row[0].get_xaxis().set_visible(False)
    row[1].set_yticklabels([])
    row[1].set_xticklabels([])
    row[1].get_yaxis().set_visible(False)
    row[1].get_xaxis().set_visible(False)
    row[2].set_yticklabels([])
    row[2].set_xticklabels([])
    row[2].get_yaxis().set_visible(False)
    row[2].get_xaxis().set_visible(False)

row = 0

def train_model(model_type, X_train, Y_train, sampling, grid):
    if sampling != "no":
        sampler = RandomOverSampler() if sampling == "over" else RandomUnderSampler()
        X_train, Y_train = sampler.fit_resample(X_train, Y_train)
    trained_model = GridSearchCV(make_pipeline(StandardScaler(), model_type), grid, scoring="f1")
    trained_model.fit(X_train, Y_train)

    print("Done training model, the following hyper parameters were used:")
    print(trained_model.best_params_)
    print("\n")
    return trained_model

def plot_metric(title, metric_plotter, trained_model, X_test, Y_test, row, col):
    plot_configuration = metric_plotter(trained_model, X_test, Y_test)
    plot_configuration.plot(ax=subs_metrics[row][col], name=title)


def get_metrics(model, X_test, Y_test):
    threshs = []
    prec = []
    rec = []
    f1s = []
    for threshold in np.arange(0, 1, 0.1):
        prediction = (model.predict_proba(X_test)[:, 1] > threshold).astype('float')
        threshs.append(threshold)
        prec.append(precision_score(Y_test, prediction, zero_division=0))
        rec.append(recall_score(Y_test, prediction))
        f1s.append(f1_score(Y_test, prediction))

    df = pd.DataFrame([prec, rec, f1s], index=["Precision", "Recall", "F1"])
    df.columns = threshs
    return df

models = {
    "Logistic Regression": {
        "model": LogisticRegression(solver='liblinear', max_iter=10000),
        "grid": {
            'logisticregression__penalty': ['l1','l2'],
            'logisticregression__C': [0.001,0.01,0.1,1,10,100,1000],
        }
    },
    "KNN": {
        "model": KNeighborsClassifier(),
        "grid": {"kneighborsclassifier__n_neighbors": range(1, 10)}
    },
    "Decision Tree": {
        "model": DecisionTreeClassifier(),
        "grid": {
            'decisiontreeclassifier__max_leaf_nodes': list(range(2, 100)),
            'decisiontreeclassifier__min_samples_split': [2, 3, 4]
        }
    }
}

for model_name, params in models.items():
    # Split the data for training and validation
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y)
    
    # Train the model with different sampling strategies
    print("Training: " + model_name)
    print("will preform grid-search for parameters, optimising for F1 score")
    print("====================\n\n")
    model_over_sampling = train_model(params["model"], X_train, Y_train, "over", params["grid"])
    print(get_metrics(model_over_sampling, X_test, Y_test))
    get_metrics(model_over_sampling, X_test, Y_test).to_html(open("./tables/" + model_name + "-over.html", "w"))
    print("====================")

    model_under_sampling = train_model(params["model"], X_train, Y_train, "under", params["grid"])
    print(get_metrics(model_under_sampling, X_test, Y_test))
    get_metrics(model_over_sampling, X_test, Y_test).to_html(open("./tables/" + model_name + "-under.html", "w"))
    print("====================")

    model_no_sampling = train_model(params["model"], X_train, Y_train, "no", params["grid"])
    print(get_metrics(model_no_sampling, X_test, Y_test))
    get_metrics(model_over_sampling, X_test, Y_test).to_html(open("./tables/" + model_name + "-no.html", "w"))
    print("====================")

    # Report confusion matrices
    confusion_plot_no = plot_confusion_matrix(model_no_sampling, X_test, Y_test)
    confusion_plot_no.plot(ax=subs_confusion[row][0])

    confusion_plot_over = plot_confusion_matrix(model_over_sampling, X_test, Y_test)
    confusion_plot_over.plot(ax=subs_confusion[row][1])

    confusion_plot_under = plot_confusion_matrix(model_under_sampling, X_test, Y_test)
    confusion_plot_under.plot(ax=subs_confusion[row][2])
    
    # Report precision recall curve
    plot_metric('no sampling', plot_precision_recall_curve, model_no_sampling, X_test, Y_test, row, 0)
    plot_metric('over sampling', plot_precision_recall_curve, model_over_sampling, X_test, Y_test, row, 0)
    plot_metric('under sampling', plot_precision_recall_curve, model_under_sampling, X_test, Y_test, row, 0)

    # Report ROC curve
    plot_metric('no sampling', plot_roc_curve, model_no_sampling, X_test, Y_test, row, 1)
    plot_metric('over sampling', plot_roc_curve, model_over_sampling, X_test, Y_test, row, 1)
    plot_metric('under sampling', plot_roc_curve, model_under_sampling, X_test, Y_test, row, 1)

    # Report accuracy
    prediction_no_sampling = model_no_sampling.predict(X_test)
    prediction_over_sampling = model_over_sampling.predict(X_test)
    prediction_under_sampling = model_under_sampling.predict(X_test)

    accuracy_no_sampling = accuracy_score(Y_test, prediction_no_sampling)
    accuracy_over_sample = accuracy_score(Y_test, prediction_over_sampling)
    accuracy_under_sample = accuracy_score(Y_test, prediction_under_sampling)
    subs_metrics[row][2].bar(['no sampling'], [accuracy_no_sampling], color='b')
    subs_metrics[row][2].bar(['over sampling'], [accuracy_over_sample], color='orange')
    subs_metrics[row][2].bar(['under sampling'], [accuracy_under_sample], color='g')
    subs_metrics[row][2].legend([
        "no sampling - {:.0%}".format(accuracy_no_sampling),
        "over sampling - {:.0%}".format(accuracy_over_sample),
        "under sampling - {:.0%}".format(accuracy_under_sample)
    ])

    # Add model name to plots row
    subs_metrics[row][0].set_ylabel(model_name)
    subs_confusion[row][0].set_ylabel(model_name)

    # Clear un-needed figures to coserve memory
    for fig_num in plt.get_fignums():
        if fig_confusion.number != fig_num and fig_metrics.number != fig_num:
            plt.close(fig_num)

    # Save model result to file
    pickle.dump(model_no_sampling, open('./models/' + model_name.lower().replace(' ', '_') + '_no_sampling', 'wb'))
    pickle.dump(model_over_sampling, open('./models/' + model_name.lower().replace(' ', '_') + '_over_sampling', 'wb'));
    pickle.dump(model_under_sampling, open('./models/' + model_name.lower().replace(' ', '_') + '_under_sampling', 'wb'));
    row += 1
    print("====================\n\n")

plt.show()

